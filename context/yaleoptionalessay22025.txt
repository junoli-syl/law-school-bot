As a Tech Lead, I viewed optimization as the ultimate arbiter of truth. At Aetna, I designed a predictive model to identify members most likely to engage with a digital health app. By targeting "high-propensity" users, I maximized ROI. The logic seemed sound, until a Clinical Director challenged it. "This model isn't just efficient," she argued. "It is exclusionary."
I felt a rising defensiveness. I doubled down on my projections, arguing that math is neutral and that we were simply prioritizing engagement. I was solving for solvency; she was solving for equity. She didn't fight my math; she challenged my premise. She asked me to stratify the 'low-propensity' group, which my model had ignored, by demographic data. The results were a shock. The group I was effectively "redlining" was disproportionately elderly and low-income. They weren't uninterested in health; they simply lacked the smartphones required to engage. "You are optimizing for ease, not for need," she told me. 
I did not concede immediately. However, I spent the following days re-evaluating the data strata, and the correlation between 'low propensity' and systemic disadvantage became undeniable. I realized that by strictly following past behaviors, I was automating past inequities. Shifting my focus, I worked with her and various teams to redesign the objective function, introducing a 'fairness constraint' that prioritized vulnerable populations even at the cost of statistical efficiency.
This experience taught me that "correctness" is a consensus reached through the friction of differing perspectives. Data is not objective truth; it is a historical artifact. I am applying to Yale Law to engage in these necessary discussions, ensuring that the blind spots in our data do not become blind spots in our justice system.
